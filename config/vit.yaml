# Vision Transformer (ViT) Model Configuration

model_params:
  img_size: 224
  patch_size: 16
  in_channels: 3
  num_classes: 10
  embed_dim: 768
  depth: 12
  num_heads: 12

transforms:
  img_size: 224  # ViT requires 224x224 images
  normalize: true
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]

data:
  root: 
    - 'data/Dataset'
    - '/content/dataset_cv/Dataset'
  batch_size: 16  # Smaller batch size for ViT (memory intensive)
  shuffle: true

training:
  epochs: 30
  patience: 7
  learning_rate: 0.0001  # Lower learning rate for ViT
  output_paths:
    - 'models/checkpoints/vit'
    - '/content/drive/MyDrive/computer-vision-checkpoints/vit'
