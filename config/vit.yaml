# Vision Transformer (ViT) Model Configuration

model_params:
  img_size: 224
  patch_size: 16
  in_channels: 3
  num_classes: 7
  embed_dim: 768
  depth: 12
  num_heads: 12

transforms:
  img_size: 224
  normalize: true
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]

data:
  root:
    - 'data/Dataset_combined'  # Local: use augmented balanced dataset
    - '/content/dataset_cv/Dataset_combined'  # Colab: use augmented balanced dataset
  batch_size: 16  # Smaller batch size for ViT (memory intensive)
  shuffle: true

training:
  epochs: 30
  patience: 7
  learning_rate: 0.0001  # Lower learning rate for ViT
  output_paths:
    - 'models/checkpoints/vit'
    - '/content/drive/MyDrive/computer-vision-checkpoints/vit'
