# Vision Transformer (ViT) Model Configuration - Pretrained from HuggingFace

model_params:
  model_name: "google/vit-base-patch16-224-in21k"  # Pretrained model from HuggingFace
  num_classes: 7  # Number of classes in your dataset
  freeze_backbone: false  # Set to true to only train classifier head
  dropout: 0.1  # Dropout rate for classifier head

transforms:
  img_size: 224
  normalize: true
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]

data:
  root:
    - 'data/Dataset_combined'  # Local: use augmented balanced dataset
    - '/content/dataset_cv/Dataset_combined'  # Colab: use augmented balanced dataset
  batch_size: 16  # Smaller batch size for ViT (memory intensive)
  shuffle: true
  val_split: 0.2  # 20% of data for validation

training:
  epochs: 30
  patience: 7
  learning_rate: 0.0001  # Lower learning rate for ViT
  output_paths:
    - 'models/checkpoints/vit'
    - '/content/drive/MyDrive/computer-vision-checkpoints/vit'
