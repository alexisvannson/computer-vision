Visualising the feature map of CNNs

How does attention work for CNNs ?

 SENet (Squeeze-and-Excitation Network)
  If you take a standard architecture (like ResNet) and add the SE Block (Channel Attention) I showed you earlier, the model is renamed with an "SE-" prefix.ResNet50 + Attention 
  
  
  2. CBAM (Convolutional Block Attention Module)1If you use both Spatial (where) and Channel (what) attention, the model is often described as using CBAM.2It is slightly heavier than SENet but often more accurate because it can literally mask out the background noise of an image.

4. Vision Transformers (ViT) -



Transfer learning and fine-tuning? (Resnet)

Autoencoder: An autoencoder learns efficient codings of unlabeled data

Variational autoencoder (VAE) 
Here, the latent space is not a fixed point anymore. It is a value sampled from a (gaussian) distribution with a learned mean and standard deviation.


Generative Adversarial Network (GAN)


Diffusion


DENO 